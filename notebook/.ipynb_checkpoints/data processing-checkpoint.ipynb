{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fc264b5",
   "metadata": {},
   "source": [
    "# Download the S2ORC Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55e6890",
   "metadata": {},
   "source": [
    "This notebook processes the latest release (2020-07-05) of the [Semantic Scholar Open Research Corpus](https://www.aclweb.org/anthology/2020.acl-main.447). It is split into 100 uniformly shuffled shards. \n",
    "\n",
    "The script will download each shard, filter computer science papers from it and extract the required data, then delete the superfluous data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54a18d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib\n",
    "import gzip\n",
    "import io\n",
    "import json\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c8eb66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Open raw file\n",
    "# with open(\"../dataset/full_urls.txt\") as f:\n",
    "#     lines = f.readlines()\n",
    "\n",
    "# getlines = [line[:-1] for line in lines if line.startswith('wget')]\n",
    "# getlines = getlines[2:]\n",
    "# metalines = [str(line.split()[-1]) for line in getlines if line.find('meta') != -1]\n",
    "# metalines = [line[1:-1] for line in metalines]\n",
    "# pdflines = [str(line.split()[-1]) for line in getlines if line.find('meta') == -1]\n",
    "# pdflines = [line[1:-1] for line in pdflines]\n",
    "\n",
    "# df = pd.DataFrame(list(zip(metalines, pdflines)), columns=['metadata', 'pdf_parses'])\n",
    "# df.to_csv(os.path.join(ROOT, 'ss_urls.csv'), index=None)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60bad67c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metadata</th>\n",
       "      <th>pdf_parses</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://ai2-s2-s2orc.s3.amazonaws.com/20200705...</td>\n",
       "      <td>https://ai2-s2-s2orc.s3.amazonaws.com/20200705...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://ai2-s2-s2orc.s3.amazonaws.com/20200705...</td>\n",
       "      <td>https://ai2-s2-s2orc.s3.amazonaws.com/20200705...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://ai2-s2-s2orc.s3.amazonaws.com/20200705...</td>\n",
       "      <td>https://ai2-s2-s2orc.s3.amazonaws.com/20200705...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://ai2-s2-s2orc.s3.amazonaws.com/20200705...</td>\n",
       "      <td>https://ai2-s2-s2orc.s3.amazonaws.com/20200705...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://ai2-s2-s2orc.s3.amazonaws.com/20200705...</td>\n",
       "      <td>https://ai2-s2-s2orc.s3.amazonaws.com/20200705...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            metadata  \\\n",
       "0  https://ai2-s2-s2orc.s3.amazonaws.com/20200705...   \n",
       "1  https://ai2-s2-s2orc.s3.amazonaws.com/20200705...   \n",
       "2  https://ai2-s2-s2orc.s3.amazonaws.com/20200705...   \n",
       "3  https://ai2-s2-s2orc.s3.amazonaws.com/20200705...   \n",
       "4  https://ai2-s2-s2orc.s3.amazonaws.com/20200705...   \n",
       "\n",
       "                                          pdf_parses  \n",
       "0  https://ai2-s2-s2orc.s3.amazonaws.com/20200705...  \n",
       "1  https://ai2-s2-s2orc.s3.amazonaws.com/20200705...  \n",
       "2  https://ai2-s2-s2orc.s3.amazonaws.com/20200705...  \n",
       "3  https://ai2-s2-s2orc.s3.amazonaws.com/20200705...  \n",
       "4  https://ai2-s2-s2orc.s3.amazonaws.com/20200705...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_url = pd.read_csv(\"../dataset/ss_urls.csv\")\n",
    "data_url.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cfa204",
   "metadata": {},
   "source": [
    "### Download the data that we need"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3135cff8",
   "metadata": {},
   "source": [
    "This part of codes are from HESCapstone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff7fde86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create required folder structure\n",
    "ROOT = '../dataset'\n",
    "CLEAN_DATA = os.path.join(ROOT, 'SS/clean')\n",
    "METADATA_INPUT_DIR = os.path.join(ROOT,'SS/metadata/raw/') \n",
    "METADATA_OUTPUT_DIR = os.path.join(ROOT,'SS/metadata/CS/')\n",
    "PDF_PARSES_INPUT_DIR = os.path.join(ROOT,'SS/pdf_parses/raw/')\n",
    "PDF_PARSES_OUTPUT_DIR = os.path.join(ROOT,'SS/pdf_parses/CS/')\n",
    "\n",
    "os.makedirs(CLEAN_DATA, exist_ok=True)\n",
    "os.makedirs(METADATA_INPUT_DIR, exist_ok=True)\n",
    "os.makedirs(METADATA_OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(PDF_PARSES_INPUT_DIR, exist_ok=True)\n",
    "os.makedirs(PDF_PARSES_OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3ef0546",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownloadBatch:\n",
    "    \"\"\"Filter raw SS dataset to extract only the papers we need\"\"\"\n",
    "\n",
    "    field = \"Computer Science\"\n",
    "\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    @staticmethod\n",
    "    def get_filename(url):\n",
    "        return os.path.basename(url.split(\"?\")[0])\n",
    "\n",
    "    def get_basename(self, url):\n",
    "        filename = self.get_filename(url).split(\".\")[0]\n",
    "        return filename\n",
    "    \n",
    "    \n",
    "    def create_batch(self, row):\n",
    "        \"\"\"Create links for each row.\"\"\"\n",
    "        input_metadata_url = row[1][\"metadata\"]\n",
    "        input_pdf_parses_url = row[1][\"pdf_parses\"]\n",
    "        batch = dict(\n",
    "            number=self.get_basename(input_metadata_url)[-1],\n",
    "            input_metadata_url=input_metadata_url,\n",
    "            input_metadata_path=os.path.join(\n",
    "                METADATA_INPUT_DIR, self.get_filename(input_metadata_url)\n",
    "            ),\n",
    "            output_metadata_path=os.path.join(\n",
    "                METADATA_OUTPUT_DIR, self.get_filename(input_metadata_url)\n",
    "            ),\n",
    "            input_pdf_parses_url=input_pdf_parses_url,\n",
    "            input_pdf_parses_path=os.path.join(\n",
    "                PDF_PARSES_INPUT_DIR, self.get_filename(input_pdf_parses_url)\n",
    "            ),\n",
    "            output_pdf_parses_path=os.path.join(\n",
    "                PDF_PARSES_OUTPUT_DIR,\n",
    "                self.get_filename(input_pdf_parses_url),\n",
    "            ),\n",
    "        )\n",
    "        return batch\n",
    "\n",
    "    def download_data(self, batch: dict):\n",
    "        \"\"\"Download the metadata and the pdf parses for the shard\"\"\"\n",
    "\n",
    "        # Download metadata\n",
    "        print(f\"Downloading raw data:\")\n",
    "        \n",
    "        if not os.path.isfile(batch[\"input_metadata_path\"]):\n",
    "            print('    Downloading metadata...')\n",
    "            urllib.request.urlretrieve(batch[\"input_metadata_url\"], batch[\"input_metadata_path\"])\n",
    "            print('    Done')\n",
    "        else:\n",
    "            print('    Metadata already downloaded')\n",
    "        \n",
    "        # Download pdf parse\n",
    "        if not os.path.isfile(batch[\"input_pdf_parses_path\"]):\n",
    "            print('    Downloading pdf parses...')\n",
    "            urllib.request.urlretrieve(batch[\"input_pdf_parses_url\"], batch[\"input_pdf_parses_path\"])\n",
    "            print('    Done')\n",
    "        else:\n",
    "            print('    PDF data already downloaded')\n",
    "        \n",
    "\n",
    "    def check_pdf_parse(self, metadata_dict):\n",
    "        \"\"\"Only keep files that have a corresponding pdf parse\"\"\"\n",
    "        if metadata_dict.get(\"has_pdf_parsed_body_text\"):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def find_topics(self, metadata_dict: dict):\n",
    "        \"\"\"Return papers that are in the field of interest\"\"\"\n",
    "        mag_field_of_study = metadata_dict[\"mag_field_of_study\"]\n",
    "        if mag_field_of_study and self.field in mag_field_of_study:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def filter_metadata(self, metadata_dict):\n",
    "        \"\"\"Filter papers of interest\"\"\"\n",
    "        # Filter papers with no pdf parse\n",
    "        if self.check_pdf_parse(metadata_dict):\n",
    "            # Filter to selected topic\n",
    "            if self.find_topics(metadata_dict):\n",
    "                return True    \n",
    "        return False\n",
    "\n",
    "    def filter_batch(self, batch: dict):\n",
    "        \"\"\"Download raw data and filter papers of interest\"\"\"\n",
    "        number = batch['number']\n",
    "        print(f'\\nProcessing shard {number}')\n",
    "        self.download_data(batch)\n",
    "\n",
    "        print(\"Filtering metadata:\")\n",
    "        paper_ids_to_keep = set()\n",
    "        with gzip.open(batch[\"input_metadata_path\"], \"rb\") as gz, open(\n",
    "            batch[\"output_metadata_path\"], \"wb\"\n",
    "        ) as f_out:\n",
    "            f = io.BufferedReader(gz)\n",
    "            for line in tqdm(f.readlines()):\n",
    "                metadata_dict = json.loads(line)\n",
    "                if self.filter_metadata(metadata_dict):\n",
    "                    paper_id = metadata_dict.get(\"paper_id\")\n",
    "                    paper_ids_to_keep.add(paper_id)\n",
    "                    f_out.write(line)\n",
    "                    \n",
    "                \n",
    "                \n",
    "        print(f'    {len(paper_ids_to_keep)} parsed papers found')\n",
    "        \n",
    "        print(\"Parsing pdfs:\")\n",
    "        with gzip.open(batch['input_pdf_parses_path'], 'rb') as gz, open(batch['output_pdf_parses_path'], 'wb') as f_out:\n",
    "            f = io.BufferedReader(gz)\n",
    "            for line in tqdm(f.readlines()):\n",
    "                metadata_dict = json.loads(line)\n",
    "                paper_id = metadata_dict['paper_id']\n",
    "                if paper_id in paper_ids_to_keep:\n",
    "                    f_out.write(line)\n",
    "        print('Done')\n",
    "                 \n",
    "    def cleanup(self, batch):\n",
    "        \"\"\"Delete the raw files to clear up space for other shards\"\"\"\n",
    "        os.remove(batch['input_metadata_path'])\n",
    "        os.remove(batch['input_pdf_parses_path'])\n",
    "\n",
    "    def __call__(self, cleanup=True):\n",
    "        for row in self.df.iterrows():\n",
    "            batch = self.create_batch(row)\n",
    "            self.filter_batch(batch)\n",
    "            if cleanup:\n",
    "                self.cleanup(batch)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69f4026d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing shard 0\n",
      "Downloading raw data:\n",
      "    Downloading metadata...\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 403: Forbidden",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4624/4040772206.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Full dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mdownload\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDownloadBatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdownload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcleanup\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4624/3947959297.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, cleanup)\u001b[0m\n\u001b[0;32m    125\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m             \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    128\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcleanup\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcleanup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4624/3947959297.py\u001b[0m in \u001b[0;36mfilter_batch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m     88\u001b[0m         \u001b[0mnumber\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'number'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'\\nProcessing shard {number}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Filtering metadata:\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4624/3947959297.py\u001b[0m in \u001b[0;36mdownload_data\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"input_metadata_path\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'    Downloading metadata...'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m             \u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murlretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"input_metadata_url\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"input_metadata_path\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'    Done'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda3\\envs\\NLP Project\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[1;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[0;32m    245\u001b[0m     \u001b[0murl_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_splittype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 247\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mcontextlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclosing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    248\u001b[0m         \u001b[0mheaders\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda3\\envs\\NLP Project\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    220\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \u001b[0mopener\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 222\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    223\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda3\\envs\\NLP Project\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    529\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_response\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 531\u001b[1;33m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    532\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    533\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda3\\envs\\NLP Project\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mhttp_response\u001b[1;34m(self, request, response)\u001b[0m\n\u001b[0;32m    638\u001b[0m         \u001b[1;31m# request was successfully received, understood, and accepted.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    639\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m200\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mcode\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 640\u001b[1;33m             response = self.parent.error(\n\u001b[0m\u001b[0;32m    641\u001b[0m                 'http', request, response, code, msg, hdrs)\n\u001b[0;32m    642\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda3\\envs\\NLP Project\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36merror\u001b[1;34m(self, proto, *args)\u001b[0m\n\u001b[0;32m    567\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhttp_err\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    568\u001b[0m             \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'default'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'http_error_default'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0morig_args\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 569\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    570\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[1;31m# XXX probably also want an abstract factory that knows when it makes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda3\\envs\\NLP Project\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    500\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 502\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    503\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    504\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda3\\envs\\NLP Project\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mhttp_error_default\u001b[1;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[0;32m    647\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mHTTPDefaultErrorHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    648\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mhttp_error_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 649\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    650\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    651\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mHTTPRedirectHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mHTTPError\u001b[0m: HTTP Error 403: Forbidden"
     ]
    }
   ],
   "source": [
    "# Full dataset\n",
    "download = DownloadBatch(data_url)\n",
    "download(cleanup=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
