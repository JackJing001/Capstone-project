{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fc264b5",
   "metadata": {},
   "source": [
    "# Download the S2ORC Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55e6890",
   "metadata": {},
   "source": [
    "This notebook processes the latest release (2020-07-05) of the [Semantic Scholar Open Research Corpus](https://www.aclweb.org/anthology/2020.acl-main.447). It is split into 100 uniformly shuffled shards. \n",
    "\n",
    "The script will download each shard, filter computer science papers from it and extract the required data, then delete the superfluous data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54a18d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import urllib\n",
    "import gzip\n",
    "import io\n",
    "import json\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8eb66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Open raw file\n",
    "# with open(\"../dataset/full_urls.txt\") as f:\n",
    "#     lines = f.readlines()\n",
    "# \n",
    "# getlines = [line[:-1] for line in lines if line.startswith('wget')]\n",
    "# getlines = getlines[2:]\n",
    "# metalines = [str(line.split()[-1]) for line in getlines if line.find('meta') != -1]\n",
    "# metalines = [line[1:-1] for line in metalines]\n",
    "# pdflines = [str(line.split()[-1]) for line in getlines if line.find('meta') == -1]\n",
    "# pdflines = [line[1:-1] for line in pdflines]\n",
    "\n",
    "# df = pd.DataFrame(list(zip(metalines, pdflines)), columns=['metadata', 'pdf_parses'])\n",
    "# df.to_csv('../dataset/ss_urls.csv', index=None)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60bad67c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metadata</th>\n",
       "      <th>pdf_parses</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://ai2-s2-s2orc.s3.amazonaws.com/20200705...</td>\n",
       "      <td>https://ai2-s2-s2orc.s3.amazonaws.com/20200705...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://ai2-s2-s2orc.s3.amazonaws.com/20200705...</td>\n",
       "      <td>https://ai2-s2-s2orc.s3.amazonaws.com/20200705...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://ai2-s2-s2orc.s3.amazonaws.com/20200705...</td>\n",
       "      <td>https://ai2-s2-s2orc.s3.amazonaws.com/20200705...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://ai2-s2-s2orc.s3.amazonaws.com/20200705...</td>\n",
       "      <td>https://ai2-s2-s2orc.s3.amazonaws.com/20200705...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://ai2-s2-s2orc.s3.amazonaws.com/20200705...</td>\n",
       "      <td>https://ai2-s2-s2orc.s3.amazonaws.com/20200705...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            metadata  \\\n",
       "0  https://ai2-s2-s2orc.s3.amazonaws.com/20200705...   \n",
       "1  https://ai2-s2-s2orc.s3.amazonaws.com/20200705...   \n",
       "2  https://ai2-s2-s2orc.s3.amazonaws.com/20200705...   \n",
       "3  https://ai2-s2-s2orc.s3.amazonaws.com/20200705...   \n",
       "4  https://ai2-s2-s2orc.s3.amazonaws.com/20200705...   \n",
       "\n",
       "                                          pdf_parses  \n",
       "0  https://ai2-s2-s2orc.s3.amazonaws.com/20200705...  \n",
       "1  https://ai2-s2-s2orc.s3.amazonaws.com/20200705...  \n",
       "2  https://ai2-s2-s2orc.s3.amazonaws.com/20200705...  \n",
       "3  https://ai2-s2-s2orc.s3.amazonaws.com/20200705...  \n",
       "4  https://ai2-s2-s2orc.s3.amazonaws.com/20200705...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_url = pd.read_csv(\"../dataset/ss_urls.csv\")\n",
    "data_url.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd719582",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cfa204",
   "metadata": {},
   "source": [
    "### Download the data that we need"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3135cff8",
   "metadata": {},
   "source": [
    "This part of codes are from HESCapstone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff7fde86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create required folder structure\n",
    "ROOT = '../dataset'\n",
    "CLEAN_DATA = os.path.join(ROOT, 'SS/clean')\n",
    "METADATA_INPUT_DIR = os.path.join(ROOT,'SS/metadata/raw/') \n",
    "METADATA_OUTPUT_DIR = os.path.join(ROOT,'SS/metadata/business/')\n",
    "PDF_PARSES_INPUT_DIR = os.path.join(ROOT,'SS/pdf_parses/raw/')\n",
    "PDF_PARSES_OUTPUT_DIR = os.path.join(ROOT,'SS/pdf_parses/business/')\n",
    "\n",
    "os.makedirs(CLEAN_DATA, exist_ok=True)\n",
    "os.makedirs(METADATA_INPUT_DIR, exist_ok=True)\n",
    "os.makedirs(METADATA_OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(PDF_PARSES_INPUT_DIR, exist_ok=True)\n",
    "os.makedirs(PDF_PARSES_OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3ef0546",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownloadBatch:\n",
    "    \"\"\"Filter raw SS dataset to extract only the papers we need\"\"\"\n",
    "\n",
    "    field = \"Business\"\n",
    "\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    @staticmethod\n",
    "    def get_filename(url):\n",
    "        return os.path.basename(url.split(\"?\")[0])\n",
    "\n",
    "    def get_basename(self, url):\n",
    "        filename = self.get_filename(url).split(\".\")[0]\n",
    "        return filename\n",
    "    \n",
    "    \n",
    "    def create_batch(self, row):\n",
    "        \"\"\"Create links for each row.\"\"\"\n",
    "        input_metadata_url = row[1][\"metadata\"]\n",
    "        input_pdf_parses_url = row[1][\"pdf_parses\"]\n",
    "        batch = dict(\n",
    "            number=self.get_basename(input_metadata_url)[-1],\n",
    "            input_metadata_url=input_metadata_url,\n",
    "            input_metadata_path=os.path.join(\n",
    "                METADATA_INPUT_DIR, self.get_filename(input_metadata_url)\n",
    "            ),\n",
    "            output_metadata_path=os.path.join(\n",
    "                METADATA_OUTPUT_DIR, self.get_filename(input_metadata_url)\n",
    "            )[:-3],\n",
    "            input_pdf_parses_url=input_pdf_parses_url,\n",
    "            input_pdf_parses_path=os.path.join(\n",
    "                PDF_PARSES_INPUT_DIR, self.get_filename(input_pdf_parses_url)\n",
    "            ),\n",
    "            output_pdf_parses_path=os.path.join(\n",
    "                PDF_PARSES_OUTPUT_DIR,\n",
    "                self.get_filename(input_pdf_parses_url),\n",
    "            )[:-3],\n",
    "        )\n",
    "        return batch\n",
    "\n",
    "    def download_data(self, batch: dict):\n",
    "        \"\"\"Download the metadata and the pdf parses for the shard\"\"\"\n",
    "\n",
    "        # Download metadata\n",
    "        print(f\"Downloading raw data:\")\n",
    "        \n",
    "        if not os.path.isfile(batch[\"input_metadata_path\"]):\n",
    "            print('    Downloading metadata...')\n",
    "            urllib.request.urlretrieve(batch[\"input_metadata_url\"], batch[\"input_metadata_path\"])\n",
    "            print('    Done')\n",
    "        else:\n",
    "            print('    Metadata already downloaded')\n",
    "        \n",
    "        # Download pdf parse\n",
    "        if not os.path.isfile(batch[\"input_pdf_parses_path\"]):\n",
    "            print('    Downloading pdf parses...')\n",
    "            urllib.request.urlretrieve(batch[\"input_pdf_parses_url\"], batch[\"input_pdf_parses_path\"])\n",
    "            print('    Done')\n",
    "        else:\n",
    "            print('    PDF data already downloaded')\n",
    "        \n",
    "\n",
    "    def check_pdf_parse(self, metadata_dict):\n",
    "        \"\"\"Only keep files that have a corresponding pdf parse\"\"\"\n",
    "        if metadata_dict.get(\"has_pdf_parsed_body_text\"):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def find_topics(self, metadata_dict: dict):\n",
    "        \"\"\"Return papers that are in the field of interest\"\"\"\n",
    "        mag_field_of_study = metadata_dict[\"mag_field_of_study\"]\n",
    "        if mag_field_of_study and self.field in mag_field_of_study:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def filter_metadata(self, metadata_dict):\n",
    "        \"\"\"Filter papers of interest\"\"\"\n",
    "        # Filter papers with no pdf parse\n",
    "        if self.check_pdf_parse(metadata_dict):\n",
    "            # Filter to selected topic\n",
    "            if self.find_topics(metadata_dict):\n",
    "                return True    \n",
    "        return False\n",
    "\n",
    "    def filter_batch(self, batch: dict):\n",
    "        \"\"\"Download raw data and filter papers of interest\"\"\"\n",
    "        number = batch['number']\n",
    "        print(f'\\nProcessing shard {number}')\n",
    "        self.download_data(batch)\n",
    "\n",
    "        print(\"Filtering metadata:\")\n",
    "        paper_ids_to_keep = set()\n",
    "        with gzip.open(batch[\"input_metadata_path\"], \"rb\") as gz, open(batch[\"output_metadata_path\"], \"wb\") as f_out:\n",
    "            f = io.BufferedReader(gz)\n",
    "            for line in tqdm(f.readlines()):\n",
    "                metadata_dict = json.loads(line)\n",
    "                if self.filter_metadata(metadata_dict):\n",
    "                    paper_id = metadata_dict.get(\"paper_id\")\n",
    "                    paper_ids_to_keep.add(paper_id)\n",
    "                    f_out.write(line)                \n",
    "                \n",
    "        print(f'    {len(paper_ids_to_keep)} parsed papers found')\n",
    "        \n",
    "        print(\"Parsing pdfs:\")\n",
    "        with gzip.open(batch['input_pdf_parses_path'], 'rb') as gz, open(batch['output_pdf_parses_path'], 'wb') as f_out:\n",
    "            f = io.BufferedReader(gz)\n",
    "            for line in tqdm(f.readlines()):\n",
    "                metadata_dict = json.loads(line)\n",
    "                paper_id = metadata_dict['paper_id']\n",
    "                if paper_id in paper_ids_to_keep:\n",
    "                    f_out.write(line)\n",
    "        print('Done')\n",
    "                 \n",
    "    def cleanup(self, batch):\n",
    "        \"\"\"Delete the raw files to clear up space for other shards\"\"\"\n",
    "        os.remove(batch['input_metadata_path'])\n",
    "        os.remove(batch['input_pdf_parses_path'])\n",
    "\n",
    "    def __call__(self, cleanup=True):\n",
    "        for row in self.df.iterrows():\n",
    "            batch = self.create_batch(row)\n",
    "            self.filter_batch(batch)\n",
    "            if cleanup:\n",
    "                self.cleanup(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69f4026d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing shard 0\n",
      "Downloading raw data:\n",
      "    Downloading metadata...\n",
      "    Done\n",
      "    Downloading pdf parses...\n",
      "    Done\n",
      "Filtering metadata:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1366661/1366661 [00:21<00:00, 63566.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    1381 parsed papers found\n",
      "Parsing pdfs:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 310736/310736 [00:58<00:00, 5304.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Full dataset\n",
    "download = DownloadBatch(data_url[:1]) # For test, use data_url to get full dataset\n",
    "download(cleanup=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
